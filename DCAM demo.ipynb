{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Deep Clustering-based Aggregation Model (DCAM)\n",
    "import numpy as np\n",
    "import deep_laa_support as dls\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#================= read data =====================\n",
    "# filename = 'default_file'\n",
    "data_all = np.load(filename +'.npz')\n",
    "print('File ' + filename + '.npz ' 'loaded.')\n",
    "user_labels = data_all['user_labels']\n",
    "true_labels = data_all['true_labels']\n",
    "category_size = data_all['category_num']\n",
    "source_num = data_all['source_num']\n",
    "feature = data_all['feature']\n",
    "_, feature_size = np.shape(feature)\n",
    "n_samples, _ = np.shape(true_labels)\n",
    "\n",
    "#================= basic parameters =====================\n",
    "# define batch size (use all samples in one batch)\n",
    "batch_size = n_samples\n",
    "cluster_num = 200\n",
    "\n",
    "if np.max(feature) <= 1 and np.min(feature) >= 0:\n",
    "    flag_node_type = 'Bernoulli'\n",
    "else:\n",
    "    flag_node_type = 'Gaussian'\n",
    "print(flag_node_type + ' output nodes are used.')\n",
    "\n",
    "#================= clustering regularizer l_cr =====================\n",
    "with tf.name_scope('regularizer'):\n",
    "    #================= reconstruction loss l_r =====================\n",
    "    with tf.variable_scope('autoencoder'):\n",
    "        x = tf.placeholder(dtype=tf.float32, shape=[batch_size, feature_size], name='x_input')\n",
    "        h1_size_encoder = int(np.floor(feature_size/2.0))\n",
    "        h2_size_encoder = 100\n",
    "        embedding_size = 40\n",
    "        h1_size_decoder = 100\n",
    "        h2_size_decoder = int(np.floor(feature_size/2.0))\n",
    "        \n",
    "        with tf.variable_scope('feature_encoder_h1'):\n",
    "            _h1_encoder, w1_encoder, b1_encoder = dls.full_connect_relu_BN(x, [feature_size, h1_size_encoder])\n",
    "        with tf.variable_scope('feature_encoder_h2'):\n",
    "            _h2_encoder, w2_encoder, b2_encoder = dls.full_connect_relu_BN(_h1_encoder, [h1_size_encoder, h2_size_encoder])\n",
    "        with tf.variable_scope('feature_encoder_mu'):\n",
    "            mu_h, w_mu_encoder, b_mu_encoder = dls.full_connect(_h2_encoder, [h2_size_encoder, embedding_size])\n",
    "        with tf.variable_scope('feature_decoder_h1'):\n",
    "            _h1_decoder, w1_decoder, b1_decoder = dls.full_connect_relu_BN(mu_h, [embedding_size, h1_size_decoder])\n",
    "        with tf.variable_scope('feature_decoder_h2'):\n",
    "            _h2_decoder, w2_decoder, b2_decoder = dls.full_connect_relu_BN(_h1_decoder, [h1_size_decoder, h2_size_decoder])\n",
    "        with tf.variable_scope('feature_decoder_rho'):\n",
    "            if flag_node_type == 'Bernoulli':\n",
    "                x_reconstr, w_rho_decoder, b_rho_decoder = dls.full_connect_sigmoid(_h2_decoder, [h2_size_decoder, feature_size])\n",
    "            elif flag_node_type == 'Gaussian':\n",
    "                x_reconstr, w_rho_decoder, b_rho_decoder = dls.full_connect(_h2_decoder, [h2_size_decoder, feature_size])\n",
    "        \n",
    "        # Bernoulli\n",
    "        loss_cross_entropy_AE = -tf.reduce_mean(tf.reduce_sum(x*tf.log(1e-10+x_reconstr) + (1.0-x)*tf.log(1e-10+(1.0-x_reconstr)), -1))\n",
    "        # Gaussian\n",
    "        loss_square_AE = 0.5 * tf.reduce_mean(tf.reduce_sum(tf.square(x_reconstr - x), -1))\n",
    "        # constraint on weights and biases\n",
    "        constraint_w_AE = 0.5 * (tf.reduce_mean(tf.square(w1_encoder)) + tf.reduce_mean(tf.square(b1_encoder))\n",
    "            + tf.reduce_mean(tf.square(w2_encoder)) + tf.reduce_mean(tf.square(b2_encoder))\n",
    "            + tf.reduce_mean(tf.square(w_mu_encoder)) + tf.reduce_mean(tf.square(b_mu_encoder))\n",
    "            + tf.reduce_mean(tf.square(w1_decoder)) + tf.reduce_mean(tf.square(b1_decoder))\n",
    "            + tf.reduce_mean(tf.square(w2_decoder)) + tf.reduce_mean(tf.square(b2_decoder))\n",
    "            + tf.reduce_mean(tf.square(w_rho_decoder)) + tf.reduce_mean(tf.square(b_rho_decoder)))\n",
    "        \n",
    "        # loss_AE\n",
    "        if flag_node_type == 'Bernoulli':\n",
    "            loss_AE = loss_cross_entropy_AE + constraint_w_AE\n",
    "        elif flag_node_type == 'Gaussian':\n",
    "            loss_AE = loss_square_AE + constraint_w_AE\n",
    "\n",
    "        # pre-train autoencoder\n",
    "        learning_rate_AE = 0.02\n",
    "        optimizer_AE_minimize = tf.train.AdamOptimizer(learning_rate=learning_rate_AE).minimize(loss_AE)\n",
    "        \n",
    "        loss_reconstr = loss_AE\n",
    "        \n",
    "    #================= clustering loss l_c =====================\n",
    "    with tf.variable_scope('clustering'):\n",
    "        mu_c = tf.get_variable('mu_c', dtype=tf.float32, initializer=tf.random_normal(shape=[cluster_num, embedding_size], mean=0, stddev=1, dtype=tf.float32))\n",
    "        mu_c_assign = tf.placeholder(dtype=tf.float32, shape=[cluster_num, embedding_size], name='mu_c_assign')\n",
    "        initialize_mu_c = tf.assign(mu_c, mu_c_assign)\n",
    "\n",
    "        mu_c_prior = tf.placeholder(dtype=tf.float32, shape=[cluster_num, embedding_size], name='mu_c_prior')\n",
    "        prior_mu_c = -0.5 * tf.reduce_mean(tf.reduce_sum(tf.square(mu_c - mu_c_prior), -1))\n",
    "        \n",
    "        # clustering distribution: q[batch_size, cluster_num]\n",
    "        square_dist = tf.reduce_sum(tf.square(tf.reshape(mu_h, [batch_size, 1, embedding_size]) - mu_c), -1)\n",
    "        nu = 1\n",
    "        _q = (1 + square_dist/nu) ** (-(nu+1)/2)\n",
    "        q = _q / (1e-10 + tf.reduce_sum(_q, -1, keepdims=True))\n",
    "\n",
    "        # set s as a constant learning target\n",
    "        fixed_s = tf.placeholder(dtype=tf.float32, shape=[batch_size, cluster_num], name='fixed_s')\n",
    "\n",
    "        loss_clustering = -tf.reduce_mean(tf.reduce_sum(fixed_s * tf.log(1e-10 + q), -1))\n",
    "        \n",
    "    # loss clustering regularizer, given \n",
    "    loss_cr = loss_clustering + loss_reconstr\n",
    "    \n",
    "    print('Clustering regularizer is constructed.')\n",
    "\n",
    "#================= likelihood p(l|y), p(y|z), and p(z|pi_z) =====================\n",
    "with tf.name_scope('likelihood'):\n",
    "    #================= p(z) =====================\n",
    "    with tf.variable_scope('p_z'):\n",
    "        # square_dist\n",
    "        _log_pi_z = -square_dist / 2\n",
    "        _log_pi_z_max = tf.reduce_max(_log_pi_z, 1, keepdims=True)\n",
    "        pi_z = tf.exp(_log_pi_z - (_log_pi_z_max + tf.log(1e-10+tf.reduce_sum(tf.exp(_log_pi_z-_log_pi_z_max), 1, keepdims=True))))\n",
    "        p_z = pi_z\n",
    "\n",
    "    #================= p(y|z) =====================\n",
    "    with tf.variable_scope('p_yz'):\n",
    "        _pi_yz = tf.get_variable('pi_yz', dtype=tf.float32, \n",
    "                                initializer=tf.random_normal(shape=[cluster_num, category_size], mean=0, stddev=1, dtype=tf.float32))\n",
    "        __pi_yz = tf.clip_by_value(_pi_yz, 1e-10, 1)\n",
    "        pi_yz = __pi_yz / tf.reduce_sum(__pi_yz, -1, keepdims=True)\n",
    "        \n",
    "        # initialize pi_yz\n",
    "        pi_yz_assign = tf.placeholder(dtype=tf.float32, shape=[cluster_num, category_size], name='pi_yz_assign')\n",
    "        initialize_pi_yz = tf.assign(_pi_yz, pi_yz_assign)\n",
    "        \n",
    "        # prior for pi_yz\n",
    "        pi_yz_prior = tf.placeholder(dtype=tf.float32, shape=[cluster_num, category_size], name='pi_yz_prior')\n",
    "        prior_pi_yz = tf.reduce_mean(tf.reduce_sum(pi_yz_prior * tf.log(1e-10+pi_yz), -1))\n",
    "        \n",
    "    #================= p(l|y) =====================\n",
    "    with tf.variable_scope('p_ly'):\n",
    "        # l_reconstr[category_size, 1, source_num*category_size]\n",
    "        output_size = source_num * category_size\n",
    "        l = tf.placeholder(dtype=tf.float32, shape=[batch_size, output_size], name='l_input')\n",
    "        weights_reconstr = tf.get_variable('weights_reconstr', dtype=tf.float32,\n",
    "                                           initializer=tf.truncated_normal(shape=[source_num, category_size, category_size], mean=0.0, stddev=.01))\n",
    "        biases_reconstr = tf.get_variable('biases_reconstr', dtype=tf.float32, initializer=tf.zeros(shape=[source_num, category_size]))\n",
    "        w = weights_reconstr\n",
    "        \n",
    "        constant_y = dls.get_constant_y(1, category_size)\n",
    "        _l_reconstr = []\n",
    "        for i in range(category_size):\n",
    "            _reconstr_tmp = constant_y[i, :, :] * w\n",
    "            _reconstr_tmp = tf.reduce_sum(_reconstr_tmp, -1)\n",
    "            _reconstr_tmp = _reconstr_tmp + biases_reconstr\n",
    "            _reconstr_tmp = tf.exp(_reconstr_tmp)\n",
    "            _reconstr_tmp = tf.div(_reconstr_tmp, tf.reduce_sum(_reconstr_tmp, -1, keepdims=True))\n",
    "            _l_reconstr.append(tf.reshape(_reconstr_tmp, [1, -1]))\n",
    "        l_reconstr = tf.stack(_l_reconstr)\n",
    "\n",
    "        prior_w_ly = -0.5 * (tf.reduce_mean(tf.square(weights_reconstr)) + tf.reduce_mean(tf.square(biases_reconstr)))\n",
    "    \n",
    "        # _tmp_cross_entropy[category_size, batch_size, output_size]\n",
    "        _tmp_cross_entropy_ly = -l * tf.log(1e-10 + l_reconstr)\n",
    "        # cross_entropy_reconstr[batch_size, category_size]\n",
    "        cross_entropy_reconstr_ly = tf.transpose(tf.reduce_sum(_tmp_cross_entropy_ly, 2))\n",
    "        log_p_ly = - cross_entropy_reconstr_ly\n",
    "    \n",
    "    #================= p(l|y) p(y|z) p(z) =====================\n",
    "    # log_p_ly[batch_size, category_size]\n",
    "    reshaped_log_p_ly = tf.reshape(log_p_ly, [batch_size, 1, category_size])\n",
    "    # p_yz[cluster_num, 1, cateogry_size]\n",
    "    reshaped_p_yz = tf.reshape(pi_yz, [1, cluster_num, category_size])\n",
    "    # p_z[batch_size, cluster_num]\n",
    "    reshaped_p_z = tf.reshape(p_z, [batch_size, cluster_num, 1])\n",
    "    # p_ly_p_yz_p_z[batch_size, cluster_num, category_size]\n",
    "    p_ly_p_yz_p_z = tf.exp(reshaped_log_p_ly) * reshaped_p_yz * reshaped_p_z\n",
    "    \n",
    "    # inferred_y[batch_size, category_size]\n",
    "    _inferred_y = tf.reduce_sum(p_ly_p_yz_p_z, 1)\n",
    "    inferred_y = _inferred_y / (1e-10 + tf.reduce_sum(_inferred_y, -1, keepdims=True))\n",
    "    \n",
    "    log_likelihood = tf.reduce_mean(tf.log(1e-10 + tf.reduce_sum(tf.reduce_sum(p_ly_p_yz_p_z, 1), -1)))\n",
    "\n",
    "    print('Log-likelihood is constructed.')\n",
    "\n",
    "#================= loss overall =====================\n",
    "with tf.name_scope('loss_overall'):\n",
    "    # loss_cr\n",
    "    # log_likelihood\n",
    "    # prior_w_ly\n",
    "    # prior_pi_yz\n",
    "    loss_DCAM = -log_likelihood + loss_cr - prior_w_ly - prior_pi_yz - prior_mu_c\n",
    "\n",
    "    learning_rate_DCAM = tf.placeholder(dtype=tf.float32, name='learning_rate_DCAM')\n",
    "    optimizer_DCAM = tf.train.AdamOptimizer(learning_rate=learning_rate_DCAM)\n",
    "    optimizer_DCAM_minimize = optimizer_DCAM.minimize(loss_DCAM)\n",
    "    reset_optimizer_DCAM = tf.variables_initializer(optimizer_DCAM.variables())\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    print('DCAM is constructed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#================= training and inference =====================\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # assign batch variables (use whole data in one batch)\n",
    "    \n",
    "    print(\"Pre-train autoencoder ...\")\n",
    "    epochs = 2000\n",
    "    for epoch in range(epochs):\n",
    "        _, monitor_loss_AE = sess.run([optimizer_AE_minimize, loss_AE], feed_dict={x:feature})\n",
    "        if epoch % 50 == 0:\n",
    "            print(\"epoch: {0} loss: {1}\".format(epoch, monitor_loss_AE))    \n",
    "    \n",
    "    #================= calculate initial parameters =====================\n",
    "    initial_mu_h = sess.run(mu_h, feed_dict={x:feature})\n",
    "    clustering_result = KMeans(n_clusters=cluster_num).fit(initial_mu_h)\n",
    "    \n",
    "    #================= save current model =====================\n",
    "    saved_path = saver.save(sess, './my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def smooth_one_hot_s(input_q, smooth=0.0):\n",
    "    max_idx = np.argmax(input_q, -1)\n",
    "    s = dls.convert_to_one_hot(max_idx, cluster_num, smooth*(cluster_num-1))\n",
    "    return s\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './my_model')\n",
    "    # initialize mu_c as clustering_result.cluster_centers_\n",
    "    _ = sess.run(initialize_mu_c, {mu_c_assign:clustering_result.cluster_centers_})\n",
    "    # initialize pi_yz as pi_yz_prior_cluster\n",
    "    pi_yz_prior_cluster = dls.get_cluster_majority_y(\n",
    "        clustering_result.labels_, user_labels, cluster_num, source_num, category_size)\n",
    "    _ = sess.run(initialize_pi_yz, {pi_yz_assign:pi_yz_prior_cluster})\n",
    "    \n",
    "    print(\"Train DCAM ...\")\n",
    "    epochs = 1000\n",
    "    learning_rate_overall = 0.0001\n",
    "    for epoch in range(epochs):\n",
    "        # calculate s\n",
    "        monitor_q = sess.run(q, feed_dict={x:feature})\n",
    "        s = smooth_one_hot_s(monitor_q, 1e-6)\n",
    "        \n",
    "        # reset optimizer given new s\n",
    "        sess.run(reset_optimizer_DCAM)\n",
    "        # optimize DCAM \n",
    "        _, monitor_loss_DCAM = sess.run(\n",
    "            [optimizer_DCAM_minimize, loss_DCAM], \n",
    "            feed_dict={x:feature, l:user_labels, \n",
    "                       learning_rate_DCAM:learning_rate_overall, \n",
    "                       fixed_s:s, \n",
    "                       pi_yz_prior:pi_yz_prior_cluster,\n",
    "                       mu_c_prior:clustering_result.cluster_centers_})\n",
    "\n",
    "        # inference\n",
    "        monitor_inferred_y = sess.run(inferred_y, feed_dict={x:feature, l:user_labels})\n",
    "        hit_num_inferred_y = dls.cal_hit_num(true_labels, monitor_inferred_y)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(\"epoch: {0} loss: {1}\".format(epoch, monitor_loss_DCAM))\n",
    "            print(\"epoch: {0} accuracy: {1}\".format(epoch, float(hit_num_inferred_y)/n_samples))\n",
    "        \n",
    "    print(\"Training DCAM is Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
